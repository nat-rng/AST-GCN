{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SVVz5AHYN2ze"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yYP1lLRjkRI"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVHhv_ItjkRK"},"outputs":[],"source":["import tensorflow as tf\n","\n","print(tf.__version__)\n","\n","tf.config.experimental.list_physical_devices(device_type=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qdk2tCJojkRM"},"outputs":[],"source":["gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","  try:\n","    # Currently, memory growth needs to be the same across GPUs\n","    for gpu in gpus:\n","      tf.config.experimental.set_memory_growth(gpu, True)\n","    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","  except RuntimeError as e:\n","    # Memory growth must be set before GPUs have been initialized\n","    print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCGW1fcrL1TV"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import scipy.sparse as sp"]},{"cell_type":"markdown","metadata":{"id":"g6zGYbq2BN3P"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wYdQgiCGsFO"},"outputs":[],"source":["def normalized_adj(adj):\n","    adj = sp.coo_matrix(adj)\n","    rowsum = np.array(adj.sum(1))\n","    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n","    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n","    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n","    normalized_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n","    normalized_adj = normalized_adj.astype(np.float32)\n","    return normalized_adj\n","\n","def calculate_laplacian(adj, lambda_max=1):\n","    adj = normalized_adj(adj + sp.eye(adj.shape[0]))\n","    adj = sp.csr_matrix(adj)\n","    adj = adj.astype(np.float32)\n","    return sparse_to_tuple(adj)\n","\n","def sparse_to_tuple(mx):\n","    mx = sp.coo_matrix(mx)\n","    coords = np.vstack((mx.row, mx.col)).transpose()\n","    L = tf.sparse.SparseTensor(coords, mx.data, mx.shape)\n","    return tf.sparse.reorder(L)\n","\n","\n","def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n","    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n","    initial = tf.random.uniform([input_dim, output_dim], minval=-init_range,\n","                            maxval=init_range, dtype=tf.float32)\n","\n","    return tf.Variable(initial, name=name)\n"]},{"cell_type":"markdown","metadata":{"id":"UUSKf5RAgyfI"},"source":["# TGCNCell - keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrBmPOrl9imy"},"outputs":[],"source":["from tensorflow.keras.regularizers import l1_l2\n","\n","class TGCNCell(tf.keras.layers.Layer):\n","    def __init__(self, num_units, adj, num_nodes, l1, l2, act=tf.nn.tanh, **kwargs):\n","        super(TGCNCell, self).__init__(**kwargs)\n","        self._act = act\n","        self._nodes = num_nodes\n","        self._units = num_units\n","        self._adj = [calculate_laplacian(adj)]\n","        self.l1 = l1\n","        self.l2 = l2\n","        self.build_weights()\n","\n","    def build_weights(self):\n","        regularizer = l1_l2(l1=self.l1, l2=self.l2)\n","        self.gates_weights = self.add_weight(shape=(self._units + 1, 2 * self._units),\n","                                             initializer='glorot_uniform',\n","                                             regularizer=regularizer,\n","                                             name='gates_weights')\n","        self.gates_biases = self.add_weight(shape=(2 * self._units,),\n","                                            initializer='zeros',\n","                                            name='gates_biases')\n","        self.candidate_weights = self.add_weight(shape=(self._units + 1, self._units),\n","                                                 initializer='glorot_uniform',\n","                                                 regularizer=regularizer,\n","                                                 name='candidate_weights')\n","        self.candidate_biases = self.add_weight(shape=(self._units,),\n","                                                initializer='zeros',\n","                                                name='candidate_biases')\n","\n","    @property\n","    def state_size(self):\n","        return self._nodes * self._units\n","\n","    @property\n","    def output_size(self):\n","        return self._units\n","\n","    def call(self, inputs, state):\n","        state = tf.reshape(state, [-1, self._nodes, self._units])\n","\n","        value = tf.nn.sigmoid(\n","            self._gc(inputs, state, 2 * self._units) + self.gates_biases)\n","\n","        r, u = tf.split(value=value, num_or_size_splits=2, axis=1)\n","        r = tf.reshape(r, [-1, self._nodes, self._units])\n","        u = tf.reshape(u, [-1, self._nodes, self._units])\n","        r_state = r * state\n","\n","        c = self._act(self._gc(inputs, r_state, self._units) + self.candidate_biases)\n","        c = tf.reshape(c, [-1, self._nodes, self._units])\n","        new_h = u * state + (1 - u) * c\n","        new_h = tf.reshape(new_h, [-1, self._nodes * self._units])\n","\n","        return new_h, new_h\n","\n","    def _gc(self, inputs, state, output_size):\n","        inputs = tf.expand_dims(inputs, 2)\n","        state = tf.reshape(state, (-1, self._nodes, self._units))\n","        x_s = tf.concat([inputs, state], axis=2)\n","        input_size = x_s.get_shape()[2]\n","        x0 = tf.transpose(x_s, perm=[1, 2, 0])\n","        x0 = tf.reshape(x0, shape=[self._nodes, -1])\n","\n","        for m in self._adj:\n","            x1 = tf.sparse.sparse_dense_matmul(m, x0)\n","\n","        x = tf.reshape(x1, shape=[self._nodes, input_size, -1])\n","        x = tf.transpose(x, perm=[2, 0, 1])\n","        x = tf.reshape(x, shape=[-1, input_size])\n","\n","        weights = self.gates_weights if output_size == 2 * self._units else self.candidate_weights\n","        x = tf.matmul(x, weights)\n","        biases = self.gates_biases if output_size == 2 * self._units else self.candidate_biases\n","        x = tf.nn.bias_add(x, biases)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2ji7yKJjkRV"},"outputs":[],"source":["import pickle\n","\n","with open(\"/data/timestep_24/trainX_timestep_24_20240108.pkl\", 'rb') as file:\n","    trainX_loaded = pickle.load(file)\n","\n","with open(\"/data/timestep_24/trainY_timestep_24_20240108.pkl\", 'rb') as file:\n","    trainY_loaded = pickle.load(file)\n","\n","with open(\"/data/timestep_24/testX_timestep_24_20240108.pkl\", 'rb') as file:\n","    testX_loaded = pickle.load(file)\n","\n","with open(\"/data/timestep_24/testY_timestep_24_20240108.pkl\", 'rb') as file:\n","    testY_loaded = pickle.load(file)"]},{"cell_type":"markdown","metadata":{"id":"GP7g-EN1jkRW"},"source":["# TGCN Model - Keras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hoqCcWoU9vdO"},"outputs":[],"source":["###### load data ######\n","road_adj = pd.read_excel(\"/data/road_connection.xlsx\")\n","adj = np.mat(road_adj)\n","\n","\n","###### numpy ######\n","trainX = np.array(trainX_loaded)\n","trainY = np.array(trainY_loaded)\n","testX = np.array(testX_loaded)\n","testY = np.array(testY_loaded)\n","\n","gru_units = 128\n","num_nodes = adj.shape[0]\n","pre_len = 12\n","seq_len = 1 # avg traffic from 30 min >> 5 mins + waitiing time 30 mins\n","# batch_size = 32\n","l1 = 0.01\n","l2 = 0.01\n","\n","class TGCNModel(tf.keras.Model):\n","    def __init__(self, num_nodes, gru_units, adj, pre_len, l1, l2):\n","        super(TGCNModel, self).__init__()\n","        self.num_nodes = num_nodes\n","        self.gru_units = gru_units\n","        self.adj = adj\n","        self.pre_len = pre_len\n","        self.tgcn_cell = TGCNCell(gru_units, adj, num_nodes, l1, l2)\n","        self.rnn = tf.keras.layers.RNN(self.tgcn_cell, return_sequences=True)\n","        self.dense_out = tf.keras.layers.Dense(num_nodes * pre_len)\n","\n","    def call(self, inputs):\n","        timesteps = inputs.shape[1]\n","        x = self.rnn(inputs)\n","        x = tf.reshape(x, [-1, self.num_nodes*timesteps*self.gru_units])\n","\n","        x = self.dense_out(x)\n","\n","        x = tf.reshape(x, [-1, self.pre_len, self.num_nodes])\n","        print(\"x shape: \", x.shape)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Y1xm6DVjkRX"},"outputs":[],"source":["# from sklearn.model_selection import TimeSeriesSplit\n","# from tensorflow.keras.callbacks import EarlyStopping\n","\n","# tscv = TimeSeriesSplit(n_splits=3)\n","# num_epochs = 100\n","# batch_size = 64\n","# all_loss_histories = []\n","# all_loss_histories_train = []\n","\n","# for tr_index, val_index in tscv.split(trainX):\n","#     print(\"TRAIN:\", tr_index, \"TEST:\", val_index)\n","#     X_tr, X_val = trainX[tr_index], trainX[val_index]\n","#     y_tr, y_val = trainY[tr_index], trainY[val_index]\n","\n","#     model = TGCNModel(num_nodes, gru_units, adj, pre_len)\n","\n","#     # Define Early Stopping callback\n","#     early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)SS\n","\n","#     model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","\n","#     history = model.fit(X_tr, y_tr,\n","#                         validation_data=(X_val, y_val),\n","#                         epochs=num_epochs, batch_size=batch_size,\n","#                         callbacks=[early_stopping], verbose=1)\n","\n","#     loss_history = history.history['val_loss']\n","#     all_loss_histories.append(loss_history)\n","#     loss_history_train = history.history['loss']\n","#     all_loss_histories_train.append(loss_history_train)\n","\n","# # Find the epoch with the minimum validation loss\n","# best_epoch = np.argmin(np.mean(all_loss_histories, axis=0))\n","\n","# # print(\"Best Epoch:\", best_epoch + 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnQnJp3ujkRX"},"outputs":[],"source":["# # Plotting training and validation loss\n","# import matplotlib.pyplot as plt\n","\n","# plt.plot(range(1, num_epochs + 1), np.mean(all_loss_histories_train, axis=0), label='Training Loss')\n","# plt.plot(range(1, num_epochs + 1), np.mean(all_loss_histories, axis=0), label='Validation Loss')\n","# plt.title('Training and Validation Loss')\n","# plt.xlabel('Epochs')\n","# plt.ylabel('Loss')\n","# plt.legend()\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEM2dbFtjkRY","outputId":"4ee186ea-6ca3-4e79-c559-f4b708003a6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(40272, 816, 6)\n","(40272, 12, 6)\n","(11472, 816, 6)\n","(11472, 12, 6)\n","Epoch 1/17\n","x shape:  (16, 12, 6)\n","x shape:  (16, 12, 6)\n"]},{"name":"stderr","output_type":"stream","text":["2024-01-09 13:00:57.576514: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n","2024-01-09 13:00:58.020116: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f0e76e6ae90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2024-01-09 13:00:58.020143: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n","2024-01-09 13:00:58.029003: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2024-01-09 13:00:58.048510: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1704780058.087522  320355 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["2517/2517 [==============================] - ETA: 0s - loss: 0.1317 - mae: 0.2382 - mse: 0.1317 - mape: 56491288.0000x shape:  (16, 12, 6)\n","2517/2517 [==============================] - 1702s 676ms/step - loss: 0.1317 - mae: 0.2382 - mse: 0.1317 - mape: 56491288.0000 - val_loss: 0.2007 - val_mae: 0.3194 - val_mse: 0.2007 - val_mape: 189139632.0000\n","Epoch 2/17\n","2517/2517 [==============================] - 1678s 667ms/step - loss: 0.0947 - mae: 0.2119 - mse: 0.0947 - mape: 52459516.0000 - val_loss: 0.1986 - val_mae: 0.3097 - val_mse: 0.1986 - val_mape: 187725808.0000\n","Epoch 3/17\n","2517/2517 [==============================] - 1689s 671ms/step - loss: 0.0905 - mae: 0.2063 - mse: 0.0905 - mape: 50703020.0000 - val_loss: 0.1844 - val_mae: 0.3004 - val_mse: 0.1844 - val_mape: 177316800.0000\n","Epoch 4/17\n","2517/2517 [==============================] - 1710s 679ms/step - loss: 0.0877 - mae: 0.2021 - mse: 0.0877 - mape: 49773304.0000 - val_loss: 0.1815 - val_mae: 0.2964 - val_mse: 0.1815 - val_mape: 175179744.0000\n","Epoch 5/17\n","2517/2517 [==============================] - 1678s 667ms/step - loss: 0.0849 - mae: 0.1976 - mse: 0.0849 - mape: 49158828.0000 - val_loss: 0.1799 - val_mae: 0.2957 - val_mse: 0.1799 - val_mape: 172812224.0000\n","Epoch 6/17\n","2517/2517 [==============================] - 1685s 669ms/step - loss: 0.0829 - mae: 0.1946 - mse: 0.0829 - mape: 48740852.0000 - val_loss: 0.1693 - val_mae: 0.3032 - val_mse: 0.1693 - val_mape: 160309408.0000\n","Epoch 7/17\n","2517/2517 [==============================] - 1685s 670ms/step - loss: 0.0812 - mae: 0.1921 - mse: 0.0812 - mape: 48261452.0000 - val_loss: 0.1675 - val_mae: 0.2878 - val_mse: 0.1675 - val_mape: 164749696.0000\n","Epoch 8/17\n"," 964/2517 [==========>...................] - ETA: 15:46 - loss: 0.0796 - mae: 0.1900 - mse: 0.0796 - mape: 47996248.0000"]}],"source":["# n_epochs = best_epoch + 1\n","batch_size = 16\n","\n","print(trainX.shape)\n","print(trainY.shape)\n","print(testX.shape)\n","print(testY.shape)\n","\n","model = TGCNModel(num_nodes, gru_units, adj, pre_len)\n","\n","model.compile(optimizer='adam', loss='mse', metrics=['mae','mse','mape'])\n","\n","model.fit(trainX, trainY, epochs = 17, batch_size=batch_size, validation_data=(testX, testY))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jecjRu6MjkRY"},"outputs":[],"source":["test_metrics = model.evaluate(testX, testY)\n","\n","loss_value = test_metrics[0]\n","mae = test_metrics[1]\n","mse = test_metrics[2]\n","mape = test_metrics[3]\n","\n","print('Loss:', loss_value)\n","print('MAE(veh):', mae)\n","print('MSE:', mse)\n","print('MAPE(%):', mape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nN5poCy6fhk_"},"outputs":[],"source":["predictions = model.predict(testX)\n","\n","mse = tf.keras.losses.MeanSquaredError()\n","rmse = tf.sqrt(mse(testY, predictions))\n","print(f\"Test RMSE: {rmse.numpy()}\")\n","\n","ss_total = tf.reduce_sum(tf.square(testY - tf.reduce_mean(testY)))\n","ss_res = tf.reduce_sum(tf.square(testY - predictions))\n","r2 = 1 - ss_res / ss_total\n","print(f\"Test R2: {r2.numpy()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"flBQXVvPjkRZ"},"outputs":[],"source":["predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLKKHDq3jkRZ"},"outputs":[],"source":["predictions.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Qtev7s5jkRZ"},"outputs":[],"source":["reshaped_predictions = predictions.reshape(-1, predictions.shape[-1])\n","reshaped_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBchgjazjkRa"},"outputs":[],"source":["df_predictions = pd.DataFrame(reshaped_predictions, columns=[f'N0{i}' for i in [1,3,4,6,8,9]])\n","df_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u2VPalNejkRb"},"outputs":[],"source":["reshaped_testY = testY.reshape(-1, predictions.shape[-1])\n","df_testY = pd.DataFrame(reshaped_testY, columns=[f'N0{i}' for i in [1,3,4,6,8,9]])\n","df_testY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4O3zo6fjkRb"},"outputs":[],"source":["df_predictions.to_csv(f\"predictions_timestep_24_batch_size_{batch_size}_20240109.csv\")\n","df_testY.to_csv(f\"testY_timestep_24_batch_size_{batch_size}_20240109.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Da9wgnt-jkRb"},"outputs":[],"source":["# model.save('/content/drive/MyDrive/Thesis/TGCN/Model')\n","# model.save_weights('/content/drive/MyDrive/Thesis/TGCN/Model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MrPlrUwfjkRd"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["GJ2yAVh0vjVC","TPHYIGROQb-6","ELgkOdN_P-kz","eyGTis2DuQTF"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
